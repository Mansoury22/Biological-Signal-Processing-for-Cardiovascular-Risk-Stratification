{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40840d77",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ce9439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchmetrics\n",
    "# !pip install imageio\n",
    "# !pip install tdqm\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install scipy\n",
    "# !pip install openpyxl\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !pip install PyWavelets\n",
    "# !pip install numpy --upgrade\n",
    "# !pip install scipy\n",
    "# !pip install pynvml\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c57ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "# Import necessary libraries.\n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import random, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import AUROC, AveragePrecision, ConfusionMatrix\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import pywt\n",
    "from scipy.signal import cwt\n",
    "import scipy\n",
    "import numpy\n",
    "import numpy as Numpy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca313649",
   "metadata": {},
   "source": [
    "# Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7c5951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Diet</th>\n",
       "      <th>Age</th>\n",
       "      <th>Condition</th>\n",
       "      <th>0</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.002</th>\n",
       "      <th>0.003</th>\n",
       "      <th>0.004</th>\n",
       "      <th>...</th>\n",
       "      <th>303.99</th>\n",
       "      <th>303.991</th>\n",
       "      <th>303.992</th>\n",
       "      <th>303.993</th>\n",
       "      <th>303.994</th>\n",
       "      <th>303.995</th>\n",
       "      <th>303.996</th>\n",
       "      <th>303.997</th>\n",
       "      <th>303.998</th>\n",
       "      <th>303.999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.1223</td>\n",
       "      <td>129.9399</td>\n",
       "      <td>129.6561</td>\n",
       "      <td>129.3926</td>\n",
       "      <td>129.0074</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F9</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.4467</td>\n",
       "      <td>130.1021</td>\n",
       "      <td>129.6764</td>\n",
       "      <td>129.1899</td>\n",
       "      <td>128.9466</td>\n",
       "      <td>...</td>\n",
       "      <td>151.2245</td>\n",
       "      <td>150.961</td>\n",
       "      <td>150.5961</td>\n",
       "      <td>150.1907</td>\n",
       "      <td>149.7853</td>\n",
       "      <td>149.3596</td>\n",
       "      <td>148.8122</td>\n",
       "      <td>148.346</td>\n",
       "      <td>147.8798</td>\n",
       "      <td>147.4946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F10</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.7845</td>\n",
       "      <td>131.4399</td>\n",
       "      <td>131.2778</td>\n",
       "      <td>130.9737</td>\n",
       "      <td>130.7102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F17</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>172.6916</td>\n",
       "      <td>173.6038</td>\n",
       "      <td>174.4146</td>\n",
       "      <td>175.1241</td>\n",
       "      <td>175.9552</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F18</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.5692</td>\n",
       "      <td>154.9138</td>\n",
       "      <td>155.4409</td>\n",
       "      <td>155.7652</td>\n",
       "      <td>156.1098</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>S29</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.2386</td>\n",
       "      <td>167.9279</td>\n",
       "      <td>168.6779</td>\n",
       "      <td>169.286</td>\n",
       "      <td>169.8739</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>S3</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.5961</td>\n",
       "      <td>150.4339</td>\n",
       "      <td>150.2515</td>\n",
       "      <td>149.988</td>\n",
       "      <td>149.7853</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>S7</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.731</td>\n",
       "      <td>146.7649</td>\n",
       "      <td>147.6771</td>\n",
       "      <td>148.6501</td>\n",
       "      <td>149.6636</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>S15</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.4884</td>\n",
       "      <td>159.8194</td>\n",
       "      <td>158.9275</td>\n",
       "      <td>158.3396</td>\n",
       "      <td>157.468</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>S23</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.1979</td>\n",
       "      <td>161.1168</td>\n",
       "      <td>160.8938</td>\n",
       "      <td>160.6505</td>\n",
       "      <td>160.4276</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 304005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0     ID   Sex Diet     Age Condition         0     0.001     0.002     0.003  \\\n",
       "1     F1  Male   NC  24 wks       NaN  130.1223  129.9399  129.6561  129.3926   \n",
       "2     F9  Male   NC  24 wks       NaN  130.4467  130.1021  129.6764  129.1899   \n",
       "3    F10  Male   NC  24 wks       NaN  131.7845  131.4399  131.2778  130.9737   \n",
       "4    F17  Male   NC  24 wks       NaN  172.6916  173.6038  174.4146  175.1241   \n",
       "5    F18  Male   NC  24 wks       NaN  154.5692  154.9138  155.4409  155.7652   \n",
       "..   ...   ...  ...     ...       ...       ...       ...       ...       ...   \n",
       "138  S29  Male   NC  12 wks       NaN  167.2386  167.9279  168.6779   169.286   \n",
       "139   S3  Male  HFD  12 wks       NaN  150.5961  150.4339  150.2515   149.988   \n",
       "140   S7  Male  HFD  12 wks       NaN   145.731  146.7649  147.6771  148.6501   \n",
       "141  S15  Male  HFD  12 wks       NaN  160.4884  159.8194  158.9275  158.3396   \n",
       "142  S23  Male  HFD  12 wks       NaN  161.1979  161.1168  160.8938  160.6505   \n",
       "\n",
       "0       0.004  ...    303.99  303.991   303.992   303.993   303.994   303.995  \\\n",
       "1    129.0074  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "2    128.9466  ...  151.2245  150.961  150.5961  150.1907  149.7853  149.3596   \n",
       "3    130.7102  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "4    175.9552  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "5    156.1098  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "..        ...  ...       ...      ...       ...       ...       ...       ...   \n",
       "138  169.8739  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "139  149.7853  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "140  149.6636  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "141   157.468  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "142  160.4276  ...       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "0     303.996  303.997   303.998   303.999  \n",
       "1         NaN      NaN       NaN       NaN  \n",
       "2    148.8122  148.346  147.8798  147.4946  \n",
       "3         NaN      NaN       NaN       NaN  \n",
       "4         NaN      NaN       NaN       NaN  \n",
       "5         NaN      NaN       NaN       NaN  \n",
       "..        ...      ...       ...       ...  \n",
       "138       NaN      NaN       NaN       NaN  \n",
       "139       NaN      NaN       NaN       NaN  \n",
       "140       NaN      NaN       NaN       NaN  \n",
       "141       NaN      NaN       NaN       NaN  \n",
       "142       NaN      NaN       NaN       NaN  \n",
       "\n",
       "[142 rows x 304005 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Excel file \"ML AP Data 2023 Full Series(Jan2).xlsx\" and load it into a DataFrame\n",
    "\n",
    "df = pd.read_excel(\"ML AP Data 2023 Full Series(Jan2).xlsx\")\n",
    "df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df.T.reset_index()\n",
    "df.columns = df.iloc[0, :]\n",
    "df = df.iloc[1:, :304005]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b02068c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000</th>\n",
       "      <td>130.1223</td>\n",
       "      <td>130.4467</td>\n",
       "      <td>131.7845</td>\n",
       "      <td>172.6916</td>\n",
       "      <td>154.5692</td>\n",
       "      <td>155.522</td>\n",
       "      <td>131.0142</td>\n",
       "      <td>126.5749</td>\n",
       "      <td>148.0217</td>\n",
       "      <td>155.6233</td>\n",
       "      <td>...</td>\n",
       "      <td>136.5482</td>\n",
       "      <td>120.8382</td>\n",
       "      <td>156.1301</td>\n",
       "      <td>160.6708</td>\n",
       "      <td>122.845</td>\n",
       "      <td>167.2386</td>\n",
       "      <td>150.5961</td>\n",
       "      <td>145.731</td>\n",
       "      <td>160.4884</td>\n",
       "      <td>161.1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.001</th>\n",
       "      <td>129.9399</td>\n",
       "      <td>130.1021</td>\n",
       "      <td>131.4399</td>\n",
       "      <td>173.6038</td>\n",
       "      <td>154.9138</td>\n",
       "      <td>155.2382</td>\n",
       "      <td>130.5075</td>\n",
       "      <td>126.2911</td>\n",
       "      <td>147.7784</td>\n",
       "      <td>156.0085</td>\n",
       "      <td>...</td>\n",
       "      <td>137.2375</td>\n",
       "      <td>120.453</td>\n",
       "      <td>155.8666</td>\n",
       "      <td>159.9005</td>\n",
       "      <td>122.7031</td>\n",
       "      <td>167.9279</td>\n",
       "      <td>150.4339</td>\n",
       "      <td>146.7649</td>\n",
       "      <td>159.8194</td>\n",
       "      <td>161.1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.002</th>\n",
       "      <td>129.6561</td>\n",
       "      <td>129.6764</td>\n",
       "      <td>131.2778</td>\n",
       "      <td>174.4146</td>\n",
       "      <td>155.4409</td>\n",
       "      <td>154.8733</td>\n",
       "      <td>130.1831</td>\n",
       "      <td>125.9059</td>\n",
       "      <td>147.4946</td>\n",
       "      <td>156.5558</td>\n",
       "      <td>...</td>\n",
       "      <td>138.0888</td>\n",
       "      <td>119.9462</td>\n",
       "      <td>155.6841</td>\n",
       "      <td>159.3532</td>\n",
       "      <td>122.2571</td>\n",
       "      <td>168.6779</td>\n",
       "      <td>150.2515</td>\n",
       "      <td>147.6771</td>\n",
       "      <td>158.9275</td>\n",
       "      <td>160.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.003</th>\n",
       "      <td>129.3926</td>\n",
       "      <td>129.1899</td>\n",
       "      <td>130.9737</td>\n",
       "      <td>175.1241</td>\n",
       "      <td>155.7652</td>\n",
       "      <td>154.63</td>\n",
       "      <td>129.7777</td>\n",
       "      <td>125.5613</td>\n",
       "      <td>147.2919</td>\n",
       "      <td>157.0423</td>\n",
       "      <td>...</td>\n",
       "      <td>138.9402</td>\n",
       "      <td>119.6016</td>\n",
       "      <td>155.4814</td>\n",
       "      <td>158.8464</td>\n",
       "      <td>122.0342</td>\n",
       "      <td>169.286</td>\n",
       "      <td>149.988</td>\n",
       "      <td>148.6501</td>\n",
       "      <td>158.3396</td>\n",
       "      <td>160.6505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.004</th>\n",
       "      <td>129.0074</td>\n",
       "      <td>128.9466</td>\n",
       "      <td>130.7102</td>\n",
       "      <td>175.9552</td>\n",
       "      <td>156.1098</td>\n",
       "      <td>154.4476</td>\n",
       "      <td>129.575</td>\n",
       "      <td>125.014</td>\n",
       "      <td>147.1906</td>\n",
       "      <td>157.5896</td>\n",
       "      <td>...</td>\n",
       "      <td>139.7511</td>\n",
       "      <td>119.257</td>\n",
       "      <td>155.3193</td>\n",
       "      <td>158.3802</td>\n",
       "      <td>121.7706</td>\n",
       "      <td>169.8739</td>\n",
       "      <td>149.7853</td>\n",
       "      <td>149.6636</td>\n",
       "      <td>157.468</td>\n",
       "      <td>160.4276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:05:03.995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>149.3596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:05:03.996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>148.8122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:05:03.997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>148.346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:05:03.998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>147.8798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:05:03.999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>147.4946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304000 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1         2         3         4         5    \\\n",
       "Time                                                                        \n",
       "1970-01-01 00:00:00.000  130.1223  130.4467  131.7845  172.6916  154.5692   \n",
       "1970-01-01 00:00:00.001  129.9399  130.1021  131.4399  173.6038  154.9138   \n",
       "1970-01-01 00:00:00.002  129.6561  129.6764  131.2778  174.4146  155.4409   \n",
       "1970-01-01 00:00:00.003  129.3926  129.1899  130.9737  175.1241  155.7652   \n",
       "1970-01-01 00:00:00.004  129.0074  128.9466  130.7102  175.9552  156.1098   \n",
       "...                           ...       ...       ...       ...       ...   \n",
       "1970-01-01 00:05:03.995       NaN  149.3596       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.996       NaN  148.8122       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.997       NaN   148.346       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.998       NaN  147.8798       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.999       NaN  147.4946       NaN       NaN       NaN   \n",
       "\n",
       "                              6         7         8         9         10   \\\n",
       "Time                                                                        \n",
       "1970-01-01 00:00:00.000   155.522  131.0142  126.5749  148.0217  155.6233   \n",
       "1970-01-01 00:00:00.001  155.2382  130.5075  126.2911  147.7784  156.0085   \n",
       "1970-01-01 00:00:00.002  154.8733  130.1831  125.9059  147.4946  156.5558   \n",
       "1970-01-01 00:00:00.003    154.63  129.7777  125.5613  147.2919  157.0423   \n",
       "1970-01-01 00:00:00.004  154.4476   129.575   125.014  147.1906  157.5896   \n",
       "...                           ...       ...       ...       ...       ...   \n",
       "1970-01-01 00:05:03.995       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.996       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.997       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.998       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.999       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                         ...       133       134       135       136  \\\n",
       "Time                     ...                                           \n",
       "1970-01-01 00:00:00.000  ...  136.5482  120.8382  156.1301  160.6708   \n",
       "1970-01-01 00:00:00.001  ...  137.2375   120.453  155.8666  159.9005   \n",
       "1970-01-01 00:00:00.002  ...  138.0888  119.9462  155.6841  159.3532   \n",
       "1970-01-01 00:00:00.003  ...  138.9402  119.6016  155.4814  158.8464   \n",
       "1970-01-01 00:00:00.004  ...  139.7511   119.257  155.3193  158.3802   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "1970-01-01 00:05:03.995  ...       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.996  ...       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.997  ...       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.998  ...       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.999  ...       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                              137       138       139       140       141  \\\n",
       "Time                                                                        \n",
       "1970-01-01 00:00:00.000   122.845  167.2386  150.5961   145.731  160.4884   \n",
       "1970-01-01 00:00:00.001  122.7031  167.9279  150.4339  146.7649  159.8194   \n",
       "1970-01-01 00:00:00.002  122.2571  168.6779  150.2515  147.6771  158.9275   \n",
       "1970-01-01 00:00:00.003  122.0342   169.286   149.988  148.6501  158.3396   \n",
       "1970-01-01 00:00:00.004  121.7706  169.8739  149.7853  149.6636   157.468   \n",
       "...                           ...       ...       ...       ...       ...   \n",
       "1970-01-01 00:05:03.995       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.996       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.997       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.998       NaN       NaN       NaN       NaN       NaN   \n",
       "1970-01-01 00:05:03.999       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                              142  \n",
       "Time                               \n",
       "1970-01-01 00:00:00.000  161.1979  \n",
       "1970-01-01 00:00:00.001  161.1168  \n",
       "1970-01-01 00:00:00.002  160.8938  \n",
       "1970-01-01 00:00:00.003  160.6505  \n",
       "1970-01-01 00:00:00.004  160.4276  \n",
       "...                           ...  \n",
       "1970-01-01 00:05:03.995       NaN  \n",
       "1970-01-01 00:05:03.996       NaN  \n",
       "1970-01-01 00:05:03.997       NaN  \n",
       "1970-01-01 00:05:03.998       NaN  \n",
       "1970-01-01 00:05:03.999       NaN  \n",
       "\n",
       "[304000 rows x 142 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create intermediary dataframes separating data and metadata to facilitate\n",
    "#further processing\n",
    "df_data = df.iloc[:, 5:].T.reset_index()\n",
    "df_data = df_data.rename(columns={0:\"Time\"})\n",
    "df_data.Time = pd.to_datetime(df_data.Time, unit=\"s\")\n",
    "df_data.index = df_data.Time\n",
    "df_data = df_data.iloc[:, 1:]\n",
    "df_meta = df.iloc[:, :5].rename(columns={\"Condition\":\"Surgery\"})\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcb6d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Diet</th>\n",
       "      <th>Age</th>\n",
       "      <th>Surgery</th>\n",
       "      <th>SexDiet</th>\n",
       "      <th>AgeDiet</th>\n",
       "      <th>SexAge</th>\n",
       "      <th>SexAgeDiet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleNC</td>\n",
       "      <td>24 wksNC</td>\n",
       "      <td>Male24 wks</td>\n",
       "      <td>Male24 wksNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F9</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleNC</td>\n",
       "      <td>24 wksNC</td>\n",
       "      <td>Male24 wks</td>\n",
       "      <td>Male24 wksNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F10</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleNC</td>\n",
       "      <td>24 wksNC</td>\n",
       "      <td>Male24 wks</td>\n",
       "      <td>Male24 wksNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F17</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleNC</td>\n",
       "      <td>24 wksNC</td>\n",
       "      <td>Male24 wks</td>\n",
       "      <td>Male24 wksNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F18</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>24 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleNC</td>\n",
       "      <td>24 wksNC</td>\n",
       "      <td>Male24 wks</td>\n",
       "      <td>Male24 wksNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>S29</td>\n",
       "      <td>Male</td>\n",
       "      <td>NC</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleNC</td>\n",
       "      <td>12 wksNC</td>\n",
       "      <td>Male12 wks</td>\n",
       "      <td>Male12 wksNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>S3</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleHFD</td>\n",
       "      <td>12 wksHFD</td>\n",
       "      <td>Male12 wks</td>\n",
       "      <td>Male12 wksHFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>S7</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleHFD</td>\n",
       "      <td>12 wksHFD</td>\n",
       "      <td>Male12 wks</td>\n",
       "      <td>Male12 wksHFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>S15</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleHFD</td>\n",
       "      <td>12 wksHFD</td>\n",
       "      <td>Male12 wks</td>\n",
       "      <td>Male12 wksHFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>S23</td>\n",
       "      <td>Male</td>\n",
       "      <td>HFD</td>\n",
       "      <td>12 wks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MaleHFD</td>\n",
       "      <td>12 wksHFD</td>\n",
       "      <td>Male12 wks</td>\n",
       "      <td>Male12 wksHFD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0     ID   Sex Diet     Age Surgery  SexDiet    AgeDiet      SexAge  \\\n",
       "1     F1  Male   NC  24 wks     NaN   MaleNC   24 wksNC  Male24 wks   \n",
       "2     F9  Male   NC  24 wks     NaN   MaleNC   24 wksNC  Male24 wks   \n",
       "3    F10  Male   NC  24 wks     NaN   MaleNC   24 wksNC  Male24 wks   \n",
       "4    F17  Male   NC  24 wks     NaN   MaleNC   24 wksNC  Male24 wks   \n",
       "5    F18  Male   NC  24 wks     NaN   MaleNC   24 wksNC  Male24 wks   \n",
       "..   ...   ...  ...     ...     ...      ...        ...         ...   \n",
       "138  S29  Male   NC  12 wks     NaN   MaleNC   12 wksNC  Male12 wks   \n",
       "139   S3  Male  HFD  12 wks     NaN  MaleHFD  12 wksHFD  Male12 wks   \n",
       "140   S7  Male  HFD  12 wks     NaN  MaleHFD  12 wksHFD  Male12 wks   \n",
       "141  S15  Male  HFD  12 wks     NaN  MaleHFD  12 wksHFD  Male12 wks   \n",
       "142  S23  Male  HFD  12 wks     NaN  MaleHFD  12 wksHFD  Male12 wks   \n",
       "\n",
       "0       SexAgeDiet  \n",
       "1     Male24 wksNC  \n",
       "2     Male24 wksNC  \n",
       "3     Male24 wksNC  \n",
       "4     Male24 wksNC  \n",
       "5     Male24 wksNC  \n",
       "..             ...  \n",
       "138   Male12 wksNC  \n",
       "139  Male12 wksHFD  \n",
       "140  Male12 wksHFD  \n",
       "141  Male12 wksHFD  \n",
       "142  Male12 wksHFD  \n",
       "\n",
       "[142 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new columns for multiclass classification\n",
    "df_meta[\"SexDiet\"] = df_meta.Sex + df_meta.Diet\n",
    "df_meta[\"AgeDiet\"] = df_meta.Age + df_meta.Diet\n",
    "df_meta[\"SexAge\"] = df_meta.Sex + df_meta.Age\n",
    "df_meta[\"SexAgeDiet\"] = df_meta.Sex + df_meta.Age + df_meta.Diet\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8450eea",
   "metadata": {},
   "source": [
    "# Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3e8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create directories\n",
    "##############\n",
    "'''\n",
    "REMEMBER TO CREATE A FOLDER THAT CONTAINS ALL SCALOGRAMS NOT DIVIDED BY CATEGORY\n",
    "THOSE WILL BE USED FOR SALIENCY-SCALOGRAMS CONCATENATION\n",
    "'''\n",
    "################\n",
    "def CrtDir (grp, col, exp):\n",
    "    '''\n",
    "    create the folders required for experimentation\n",
    "    grp: Classification (e.g. Age)\n",
    "    col: items inside classification (12 weeks, 24 weeks)\n",
    "    exp: experiment number (in frequency)\n",
    "    '''\n",
    "    files = [\"scal\", \"scal-cropped\"]\n",
    "    main = f'Scalograms2/freq_{exp}_({col})'\n",
    "    if os.path.exists(main):\n",
    "        shutil.rmtree(main)\n",
    "        os.mkdir(main)\n",
    "    for f in files:\n",
    "        os.makedirs(os.path.join(main,f'{f}'))\n",
    "        if f == \"scal-cropped\":\n",
    "            os.makedirs(os.path.join(main,f'{f}', 'train'))\n",
    "            os.makedirs(os.path.join(main,f'{f}', 'test'))\n",
    "            os.makedirs(os.path.join(main,f'{f}', 'val'))\n",
    "            os.makedirs(os.path.join(main,f'{f}', 'all_scal'))\n",
    "            \n",
    "\n",
    "        for g in grp:\n",
    "            if f == \"scal-cropped\":\n",
    "                path_train = os.path.join(main,f'{f}', 'train',f'{g}')\n",
    "                path_test = os.path.join(main,f'{f}', 'test',f'{g}')\n",
    "                path_val = os.path.join(main,f'{f}', 'val',f'{g}')\n",
    "                os.makedirs(path_train)\n",
    "                os.makedirs(path_test)\n",
    "                os.makedirs(path_val)\n",
    "            main_path = os.path.join(main,f'{f}',f'{g}')\n",
    "            os. makedirs(main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313652ea",
   "metadata": {},
   "source": [
    "# Slicing Scalograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ccef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_scal(grp, col, freq, df, exp, w=16.67):\n",
    "    '''\n",
    "      Slice timeseries data into bins with a bin size w,\n",
    "      and compute the scalogram for each bin.\n",
    "      ------------------------------------\n",
    "      grp: the values to be classified\n",
    "      col: name of the variable to be classified\n",
    "      freq: sampling rate in the timeseries data\n",
    "      df: dataframe containing timeseries data\n",
    "      exp: experiment number. Used to name folders\n",
    "      w: bin size in seconds\n",
    "      '''\n",
    "    # Change W to represent the number of points depending on the sampling rate\n",
    "    w = int(w * freq)\n",
    "\n",
    "    for g in grp:\n",
    "\n",
    "        # slice the dataframe to include only the variable being classified\n",
    "        y = df[(df[col] == g)]\n",
    "\n",
    "        # Naming convention; names images numerically.\n",
    "        j = 0\n",
    "\n",
    "        for i in range(y.shape[0]):\n",
    "            x = y.iloc[i, 9:]\n",
    "\n",
    "            for z in range(int(x.shape[0] / w)):\n",
    "\n",
    "                # if there are not enough points to create a new scalogram with the current window, discard the remaining points\n",
    "                if x.iloc[int(z * w): int((z + 1) * w)].isna().values.any() == True:\n",
    "                    break\n",
    "\n",
    "                # Add zeros to pad the image numbers so that sorting works properly\n",
    "                if len(str(j)) == 1:\n",
    "                    h = \"000\" + str(j)\n",
    "                elif len(str(j)) == 2:\n",
    "                    h = \"00\" + str(j)\n",
    "                elif len(str(j)) == 3:\n",
    "                    h = \"0\" + str(j)\n",
    "                else:\n",
    "                    h = str(j)\n",
    "                \n",
    "                # the Wavelet Transform computes the inner product of the signal with a scaled  \n",
    "                # and translated version of the wavelet function over all possible scales and time positions.\n",
    "                \n",
    "                # Create a new scalogram with a bin size w and sampling rate freq\n",
    "                signal_slice = x.iloc[int(z * w): int((z + 1) * w)]\n",
    "                scales = np.arange(1, 128)  # Choose appropriate scales for the scalogram\n",
    "                scalogram, frequencies = pywt.cwt(signal_slice, scales, 'morl')\n",
    "\n",
    "                \n",
    "                # This results in a two-dimensional representation, where one axis represents time, the other represents scale, \n",
    "                # and the value at each point represents the strength of the correlation\n",
    "                \n",
    "                # Plot and save the scalogram\n",
    "                plt.figure(figsize=(6.4, 4.8))\n",
    "                plt.imshow(np.abs(scalogram), aspect='auto', cmap='viridis')\n",
    "                plt.colorbar(label='Magnitude')\n",
    "                plt.xlabel('Time (samples)')\n",
    "                plt.ylabel('Scale')\n",
    "                plt.title(f'Scalogram - {col} ({g})')\n",
    "                plt.savefig(f'Scalograms2/freq_{freq}_({col})/scal/{g}/{g + str(h)}.png')\n",
    "\n",
    "                # Clear old figure away from memory\n",
    "                plt.close()\n",
    "                j = j + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9dc403",
   "metadata": {},
   "source": [
    "# Cropping all Scalograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc016edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cropping all scalograms\n",
    "def crop_scal(grp, exp):\n",
    "    '''\n",
    "      Crop scalograms to remove all white spaces and axes, leaving behind only the plot behind\n",
    "      grp: The values being classified. Used here to retrieve images and save them to the appropriate directory\n",
    "      exp: Experiment number. Used to name folders\n",
    "    '''\n",
    "    \n",
    "      # Box defines the cropping parameters to perform on the plot. Sometimes, the default plot size changes from IDE to another,\n",
    "      # Should you change your IDE, check a single trial in the cell above to see if the plot is cropped properly\n",
    "      # The format of the box is [left, upper, right, lower]\n",
    "      # Example: [left, upper, right, lower] for a 6.4 x 4.8 inch image (aspect ratio 4:3)\n",
    "\n",
    "    box = [81, 59, 477, 427]\n",
    "    \n",
    "    for g in grp:\n",
    "        path = os.path.join(f'Scalograms2/freq_{exp}_({col})/scal', f'{g}')\n",
    "        j = 0  \n",
    "        for filename in os.listdir(path):\n",
    "\n",
    "            scal  =  os.path.join(path, f\"{filename}\")\n",
    "\n",
    "            if len(str(j)) == 1:\n",
    "                h = \"000\" + str(j)\n",
    "            elif len(str(j)) == 2:\n",
    "                h = \"00\" + str(j)\n",
    "            elif len(str(j)) == 3:\n",
    "                h = \"0\" + str(j)\n",
    "            else:\n",
    "                h = str(j)\n",
    "                \n",
    "            #Open image, crop it, and save to the appropriate directory\n",
    "            im = Image.open(scal)\n",
    "            region = im.crop(box)\n",
    "\n",
    "            #Save in the respective group folder\n",
    "            region.save(f'Scalograms2/freq_{exp}_({col})/scal-cropped/{g}/{g+str(h)}.png')\n",
    "            #And in the folder containing all scalograms\n",
    "            region.save(f'Scalograms2/freq_{exp}_({col})/scal-cropped/all_scal/{g+str(h)}.png')\n",
    "            j = j+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a7279",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9bbf232",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# @title Helper functions (run me)\n",
    "\n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"WARNING: For this notebook to perform best, \"\n",
    "              \"if possible, in the menu under `Runtime` -> \"\n",
    "              \"`Change runtime type.`  select `GPU` \")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "#  Plotting function.\n",
    "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc, freq, col, rn):\n",
    "    epochs = len(train_loss)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "    ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Epoch vs Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "    ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Epoch vs Accuracy')\n",
    "    ax2.legend()\n",
    "    fig.set_size_inches(15.5, 5.5)\n",
    "    fig.savefig(f'Figures/Experiments/Freq_{str(freq)}_{col}_{str(rn)}.png')\n",
    "    plt.clf()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a78df6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled in this notebook.\n"
     ]
    }
   ],
   "source": [
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a914b627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system?True\n",
      "CUDA version: 12.1\n",
      "ID of current CUDA device:0\n",
      "Name of current CUDA device:NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9758b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaded92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset GPU allocation to 0 to resolve out of memory errors\n",
    "# Empties CUDA cache to deal with out of memory errors\n",
    "def report_gpu():\n",
    "    #print(torch.cuda.list_gpu_processes())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df029c",
   "metadata": {},
   "source": [
    "# Create folder with training, testing and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece66dcd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# @title Create folder with training, testing and validation data.\n",
    "\n",
    "def splt_scal(grp, exp):\n",
    "    '''  \n",
    "    Create train, test, and validation folders, and shuffle images into them (ratios 8:1:1)\n",
    "    '''\n",
    "    scalograms_dir = [f'Scalograms2/freq_{exp}_({col})/scal-cropped']\n",
    "    folder_names = [f'Scalograms2/freq_{exp}_({col})/scal-cropped/train',\n",
    "                    f'Scalograms2/freq_{exp}_({col})/scal-cropped/test',\n",
    "                    f'Scalograms2/freq_{exp}_({col})/scal-cropped/val']\n",
    "\n",
    "\n",
    "    #Create train test val folders \n",
    "    for f in folder_names:\n",
    "        if os.path.exists(f):\n",
    "            shutil.rmtree(f)\n",
    "            os.mkdir(f)\n",
    "        else:\n",
    "            os.mkdir(f)\n",
    "\n",
    "\n",
    "      #Group 1 copying\n",
    "    for g in grp:\n",
    "        # find all images & split in train, test, and validation\n",
    "        src_file_paths= []\n",
    "        for im in glob.glob(os.path.join(scalograms_dir[0], f'{g}',\"*.png\"), recursive=True):\n",
    "            src_file_paths.append(im)\n",
    "        random.shuffle(src_file_paths)\n",
    "        #src_file_paths = sorted(src_file_paths) #try sorted data\n",
    "\n",
    "        train_files = src_file_paths[:int(len(src_file_paths) * 0.8)] #Take 80% of files into training\n",
    "        val_files = src_file_paths[int(len(src_file_paths) * 0.8):int(len(src_file_paths) * 0.9)] #10% into validation\n",
    "        test_files = src_file_paths[int(len(src_file_paths) * 0.9):] #And 10% into testing\n",
    "\n",
    "        #  make destination folders for train and test images\n",
    "        for f in folder_names:\n",
    "            if not os.path.exists(os.path.join(f , f\"{g}\")):\n",
    "                os.mkdir(os.path.join(f , f\"{g}\"))\n",
    "\n",
    "        # copy training and testing images over\n",
    "        for f in train_files:\n",
    "            shutil.copy(f, os.path.join(os.path.join(folder_names[0]) + '/', f\"{g}\", os.path.split(f)[1]))\n",
    "        for f in test_files:\n",
    "            shutil.copy(f, os.path.join(os.path.join(folder_names[1]) + '/',f\"{g}\", os.path.split(f)[1]))\n",
    "        for f in val_files:\n",
    "            shutil.copy(f, os.path.join(os.path.join(folder_names[2]) + '/',f\"{g}\", os.path.split(f)[1]))\n",
    "    return folder_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ca1d3",
   "metadata": {},
   "source": [
    "# Data Loading Scalograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854070d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data loading\n",
    "def load_scal(folder_names, batch_size = 25, shuffle=True):\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        folder_names[0],\n",
    "        transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
    "\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        folder_names[2],\n",
    "        transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ]))\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(\n",
    "        folder_names[1],\n",
    "        transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ]))\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ce8b1",
   "metadata": {},
   "source": [
    "# Make a CNN & train it to predict genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d371caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Make a CNN & train it to predict genres.\n",
    "\n",
    "class b2b_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Intitalize neural net layers\"\"\"\n",
    "        \n",
    "        super(b2b_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1, padding=1)\n",
    "#         self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=2, stride=1, padding=1)\n",
    "#         self.conv7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=2, stride=1, padding=1)\n",
    "#         self.conv8 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=2, stride=1, padding=1)\n",
    "#         self.conv9 = nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=2, stride=1, padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=2048, out_channels=4096, kernel_size=2, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=19968, out_features=len(grp))\n",
    "#         self.fc1 = nn.Linear(in_features=6272, out_features=len(grp))\n",
    "    \n",
    "        self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "#         self.batchnorm6 = nn.BatchNorm2d(num_features=256)\n",
    "#         self.batchnorm7 = nn.BatchNorm2d(num_features=512)\n",
    "#         self.batchnorm8 = nn.BatchNorm2d(num_features=1024)\n",
    "#         self.batchnorm9 = nn.BatchNorm2d(num_features=2048)\n",
    "#         self.batchnorm10 = nn.BatchNorm2d(num_features=4096)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.4, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Conv layer 1.\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Conv layer 2.\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Conv layer 3.\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Conv layer 4.\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Conv layer 5.\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "#         # Conv layer 6.\n",
    "#         x = self.conv6(x)\n",
    "#         x = self.batchnorm6(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "#         # Conv layer 7.\n",
    "#         x = self.conv7(x)\n",
    "#         x = self.batchnorm7(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "#         # Conv layer 8.\n",
    "#         x = self.conv8(x)\n",
    "#         x = self.batchnorm8(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        \n",
    "#         # Conv layer 9.\n",
    "#         x = self.conv9(x)\n",
    "#         x = self.batchnorm9(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "#         # Conv layer 10.\n",
    "#         x = self.conv10(x)\n",
    "#         x = self.batchnorm10(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "\n",
    "        # Fully connected layer 1.\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ccd25",
   "metadata": {},
   "source": [
    "# Model Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f221c52",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# @title Model Handler\n",
    "\n",
    "class ModelHandler():\n",
    "    def __init__(self, model, device, epochs, learning_rate=0.00005):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.criterion =  nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def train(self, train_loader, validation_loader):\n",
    "        train_loss, validation_loss = [], []\n",
    "        train_acc, validation_acc = [], []\n",
    "        #with (range(self.epochs)) as tepochs:\n",
    "        with tqdm(range(self.epochs), unit='epoch') as tepochs:\n",
    "            tepochs.set_description('Training')\n",
    "            for epoch in tepochs:\n",
    "                self.model.train()\n",
    "\n",
    "                # keep track of the running loss\n",
    "                running_loss = 0.\n",
    "                correct, total = 0, 0\n",
    "\n",
    "                for data, target in train_loader:\n",
    "#                   # getting the training set\n",
    "#                   data, target = data.to(device), target.to(device)\n",
    "#                   # Get the model output (call the model with the data from this batch)\n",
    "#                   output = self.model(data)\n",
    "#                   # Zero the gradients out)\n",
    "#                   self.optimizer.zero_grad()\n",
    "#                   # Get the Loss\n",
    "#                   loss = self.criterion(output, target)\n",
    "#                   # Calculate the gradients\n",
    "#                   loss.backward()\n",
    "#                   # Update the weights (using the training step of the optimizer)\n",
    "#                   self.optimizer.step()\n",
    "#                   # Zero the gradients out\n",
    "\n",
    "                    self.optimizer.zero_grad() \n",
    "\n",
    "                    #getting the training set\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    with autocast(dtype=torch.float16): \n",
    "                    # Get the model output (call the model with the data from this batch)\n",
    "                        output = self.model(data)\n",
    "                        # Get the Loss\n",
    "                        loss = self.criterion(output, target)\n",
    "                  # Calculate the gradients\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                  # Update the weights (using the training step of the optimizer)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "\n",
    "                    tepochs.set_postfix(loss=loss.item())\n",
    "                    running_loss += loss.item()  # add the loss for this batch\n",
    "\n",
    "                  # get accuracy\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "\n",
    "                # append the loss for this epoch\n",
    "                train_loss.append(running_loss/len(train_loader))\n",
    "                train_acc.append(correct/total)\n",
    "\n",
    "                # evaluate on validation data\n",
    "                self.model.eval()\n",
    "                running_loss = 0.\n",
    "                correct, total = 0, 0\n",
    "\n",
    "                for data, target in validation_loader:\n",
    "                    # getting the validation set\n",
    "                    self.optimizer.zero_grad()\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "\n",
    "                    with autocast(dtype=torch.float16):\n",
    "                        output = self.model(data)\n",
    "                        loss = self.criterion(output, target)\n",
    "                    tepochs.set_postfix(loss=loss.item())\n",
    "                    running_loss += loss.item()\n",
    "                  # get accuracy\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "\n",
    "                validation_loss.append(running_loss/len(validation_loader))\n",
    "                validation_acc.append(correct/total)\n",
    "\n",
    "        return train_loss, train_acc, validation_loss, validation_acc\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        test_loss, test_acc = 0, 0\n",
    "        outputs = torch.tensor([]).to(device)\n",
    "        targets = torch.tensor([]).to(device)\n",
    "        # with tqdm(range(self.epochs), unit='epoch') as tepochs:\n",
    "          # tepochs.set_description('Testing')\n",
    "          # evaluate on validation data\n",
    "        self.model.eval()\n",
    "        running_loss = 0.\n",
    "        correct, total = 0, 0\n",
    "        #pre = 0\n",
    "\n",
    "        for data, target in test_loader:\n",
    "          # getting the validation set\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            with autocast(dtype=torch.float16):\n",
    "                output = self.model(data)\n",
    "\n",
    "          # get accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += float(target.size(0))\n",
    "            correct += float((predicted == target).sum().item())\n",
    "            test_acc = correct/total\n",
    "            outputs = torch.cat((outputs, output), 0)\n",
    "            targets = torch.cat((targets, target), 0)\n",
    "\n",
    "        return outputs, targets, test_acc\n",
    "\n",
    "    def visualize(self, test_loader, name_dic, freq, grp, rn):\n",
    "        '''\n",
    "        Create saliency maps for the scalograms produced\n",
    "        -------\n",
    "        test_loader: loads test images into the model. \n",
    "                     Needs to be set up with batch size 1, and shuffle = False in the loadscal function\n",
    "        name_dic: dictionary created using func \"create_name_dic\" that names saliency maps after their parent scalograms\n",
    "        -------\n",
    "        #References:\n",
    "        #https://towardsdatascience.com/saliency-map-using-pytorch-68270fe45e80\n",
    "        #https://medium.datadriveninvestor.com/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4\n",
    "        #https://github.com/sijoonlee/deep_learning/blob/master/cs231n/NetworkVisualization-PyTorch.ipynb\n",
    "        '''\n",
    "        main = \"Saliency\"\n",
    "        path = os.path.join(main, f\"{freq}_{grp}_{rn}\")\n",
    "        if not os.path.exists(main):\n",
    "            os.mkdir(main)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        i = 0 \n",
    "        for X, target in test_loader:\n",
    "            X, target = X.to(device), target.to(device)\n",
    "\n",
    "            # we would run the model in evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # we need to find the gradient with respect to the input image, so we need to call requires_grad_ on it\n",
    "            X.requires_grad_()\n",
    "\n",
    "            scores = model(X)\n",
    "\n",
    "            scores = (scores.gather(1, target.view(-1, 1)).squeeze())\n",
    "\n",
    "            #backward pass\n",
    "            scores.backward()\n",
    "\n",
    "            saliency, _ = torch.max(X.grad.data.abs(),dim=1)\n",
    "\n",
    "            # code to plot the saliency map as a heatmap\n",
    "            plt.imshow(saliency[0].clone().detach().cpu(), cmap=plt.cm.hot)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.path.join(path, f\"{name_dic[i]}\"))\n",
    "            plt.clf()\n",
    "            del X, target, scores, saliency\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4f21a",
   "metadata": {},
   "source": [
    "# Creates dictionary to help with the naming of saliency images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b3544af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_dic(freq, col):\n",
    "    '''\n",
    "    Creates dicitonary to help with the naming of saliency images.\n",
    "    Used concurrently with data loader (load scal) setup with batch size 1 and shuffle = False\n",
    "    in order to produce saliency maps in the same order as test images, and name them accordingly.\n",
    "    '''\n",
    "    name_dic = {}\n",
    "    i = 0\n",
    "    path = f\"Scalograms2/freq_{str(freq)}_({col})/scal-cropped/test\"\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(os.path.join(path, folder)):\n",
    "            name_dic[i] = filedictionary \n",
    "            i += 1\n",
    "    return name_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba01dca",
   "metadata": {},
   "source": [
    "# Downsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20dce3a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#@title Downsampler\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownsample\u001b[39m(freq, df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_data\u001b[49m, meta \u001b[38;5;241m=\u001b[39m df_meta):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Downsample BP data to specified samplerate in milliseconds by taking the mean\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    It can also be used to decrease the bit rate when transmitting over a limited bandwidth or to convert to a more limited audio format.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#convert the samplerate provided (in Hz) to time in milliseconds.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_data' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Downsampler\n",
    "def downsample(freq, df = df_data, meta = df_meta):\n",
    "    '''\n",
    "    Downsample BP data to specified samplerate in milliseconds by taking the mean\n",
    "    \n",
    "    Downsampling is a process of reducing the sampling rate or resolution of a signal or image. \n",
    "    It is done to improve efficiency, reduce file size, or enhance performance. \n",
    "    It can also be used to decrease the bit rate when transmitting over a limited bandwidth or to convert to a more limited audio format.\n",
    "    '''\n",
    "    #convert the samplerate provided (in Hz) to time in milliseconds.\n",
    "    ms = 1 / freq * 1000\n",
    "    ms = round(ms, 3)\n",
    "    #Resample the dataframe using the df.resample method\n",
    "    df = df.resample(f\"{ms}ms\").mean()\n",
    "    df = pd.concat([df_meta, df.reset_index(drop=True).T], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f69fe",
   "metadata": {},
   "source": [
    "# Modeling Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6f07dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate 10 random numbers from 1 to 1000000 to be used as random seeds for the model\n",
    "# used to ensure that results are reproducible\n",
    "# In other words, using this parameter makes sure that anyone who re-runs your code will get the exact same outputs.\n",
    "#randomlist = np.random.randint(1, 1000000, (10,))\n",
    "randomlist = [200396, 544352, 524927, 635473, 609072, 680561, 227999, 156037, 535490, 208398]\n",
    "\n",
    "#Umcomment this to create an Experiments folder\n",
    "# os.makedirs('Experiments')\n",
    "exp = 1\n",
    "\n",
    "# sampling rate in the timeseries data\n",
    "freqs = [140, 120, 100, 80, 70, 60, 50, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a97bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results in a dicitonary\n",
    "\n",
    "multiclassresults = {}\n",
    "multiclassresults = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efc8a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 1\n",
    "grps = {\n",
    "        'Age': ['12 wks', '24 wks'],\n",
    "        'Sex': ['Male', 'Female'],\n",
    "        'Diet': ['HFD', 'NC'],\n",
    "        'SexDiet': ['MaleNC', 'FemaleNC', 'MaleHFD', 'FemaleHFD'],\n",
    "        'AgeDiet': ['24 wksNC', '24 wksHFD', '12 wksNC', '12 wksHFD'],\n",
    "        'SexAge': ['Male12 wks', 'Male24 wks', 'Female12 wks', 'Female24 wks'],\n",
    "        'SexAgeDiet': ['Male24 wksNC', 'Male24 wksHFD', 'Male12 wksNC', 'Male12 wksHFD',\n",
    "                       'Female24 wksNC', 'Female24 wksHFD', 'Female12 wksNC', 'Female12 wksHFD']\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72999609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989f366ff7e940a0a24c5849f39a45c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8612db1eb44046e1b1967682a5cfc788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85636d760fe3488894338f81a57c078d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4e40e7a4934d3f8faf132f57c7e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023211bc0539446793d1a3d0db387dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f93b9960bd4d81a19c03885f586e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea0c207bea42489b91f537a40d6aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col, grp in grps.items():\n",
    "    for freq in tqdm(freqs):\n",
    "        for rn in randomlist:\n",
    "            # Load your pre-trained model\n",
    "            # net = b2b_net().to(device)\n",
    "            model = b2b_net().to(device)  # Create an instance of your model class with GPU\n",
    "            model.load_state_dict(torch.load(f\"Models/{freq}_{col}_{rn}\"))  # Load the saved parameters\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            # net.load_state_dict(torch.load(f\"Models/{freq}_{col}_{rn}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79dffad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_2236\\1695168632.py:66: GradioDeprecationWarning: `capture_session` parameter is deprecated, and it has no effect\n",
      "  iface = gr.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7891\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7891/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\gradio\\routes.py\", line 534, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\gradio\\route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\gradio\\blocks.py\", line 1554, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\gradio\\blocks.py\", line 1192, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\anaconda3\\envs\\clone\\Lib\\site-packages\\gradio\\utils.py\", line 659, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_2236\\1695168632.py\", line 51, in predict_scalogram\n",
      "    input_tensor = torch.from_numpy(processed_input).unsqueeze(0).permute(0, 3, 1, 2).float()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define preprocess_input function\n",
    "def preprocess_input(input_scalogram):\n",
    "    # Load the image using PIL\n",
    "    pil_image = Image.open(input_scalogram)\n",
    "    \n",
    "    # Convert the image to the format expected by your model (e.g., RGB or grayscale)\n",
    "    pil_image = pil_image.convert(\"RGB\")  # Adjust based on your requirements\n",
    "    \n",
    "    # Resize the image if needed\n",
    "    pil_image = pil_image.resize((224, 224))  # Adjust dimensions\n",
    "    \n",
    "    # Convert PIL image to a numpy array\n",
    "    input_array = np.asarray(pil_image)\n",
    "    \n",
    "    # Flatten the input array and normalize\n",
    "    input_array = input_array.flatten() / 255.0\n",
    "    \n",
    "    # Return the processed input\n",
    "    return input_array\n",
    "\n",
    "# Define postprocess_output function\n",
    "def postprocess_output(output):\n",
    "    # Assuming output is a tuple (predicted_class, confidence)\n",
    "    predicted_class, confidence = output\n",
    "    return f\"Predicted Class: {predicted_class}, Confidence: {confidence}\"\n",
    "\n",
    "def predict_scalogram(input_scalogram):\n",
    "    if isinstance(input_scalogram, str):\n",
    "        # If it's a string, assume it's a file path\n",
    "        input_scalogram_path = input_scalogram\n",
    "    else:\n",
    "        # If it's not a string, assume it's a NumPy array\n",
    "        # You might need to convert it to an image format compatible with your model\n",
    "        input_scalogram = np.array(input_scalogram)\n",
    "        pil_image = Image.fromarray(input_scalogram)\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            pil_image.save(temp_file, format='PNG')\n",
    "            input_scalogram_path = temp_file.name\n",
    "\n",
    "    # Preprocess the input\n",
    "    processed_input = preprocess_input(input_scalogram_path)\n",
    "\n",
    "    # Convert the input to a PyTorch tensor and move to the same device as the model\n",
    "    input_tensor = torch.from_numpy(processed_input).unsqueeze(0).permute(0, 3, 1, 2).float()\n",
    "    input_tensor = input_tensor.to(device)  # Assuming device is defined\n",
    "\n",
    "    # Use the model for prediction\n",
    "    result = model(input_tensor)\n",
    "\n",
    "    # Postprocess the output\n",
    "    processed_output = postprocess_output(result)\n",
    "\n",
    "    # For demonstration, assuming a tuple (predicted_class, confidence) as output\n",
    "    return processed_output\n",
    "\n",
    "\n",
    "\n",
    "# Define Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_scalogram,\n",
    "    inputs=\"image\",  # Adjust according to your input type\n",
    "    outputs=\"text\",  # Adjust according to your output type\n",
    "    live=True,\n",
    "    title=\"Scalogram Prediction Demo\",\n",
    "    description=\"Upload a scalogram for prediction.\",\n",
    "    capture_session=True\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c16b19a",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4caaca49c643cb9d6e2867dbbace72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "\u001b[92mTrain val test splitting is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec140c2714b9401dbe29aff74f33e742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTraining is done!\n",
      "\u001b[92mTesting is done!\n",
      "0.9801587301587301\n",
      "\u001b[92mExp 1 is finally done!\n",
      "100\n",
      "\u001b[92mTrain val test splitting is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59c7b3e57244769a4d13d2bef2b3896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTraining is done!\n",
      "\u001b[92mTesting is done!\n",
      "0.9444444444444444\n",
      "\u001b[92mExp 2 is finally done!\n",
      "100\n",
      "\u001b[92mTrain val test splitting is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8402502ce2d94f29a5f72b847da12109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTraining is done!\n",
      "\u001b[92mTesting is done!\n",
      "0.9841269841269841\n",
      "\u001b[92mExp 3 is finally done!\n",
      "100\n",
      "\u001b[92mTrain val test splitting is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a735e04ddd419bbae9ce64fd026530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTraining is done!\n",
      "\u001b[92mTesting is done!\n",
      "0.9603174603174603\n",
      "\u001b[92mExp 4 is finally done!\n",
      "100\n",
      "\u001b[92mTrain val test splitting is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c4e85663c04ef4858297391c687945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m handler \u001b[38;5;241m=\u001b[39m ModelHandler(net, device, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m0.00005\u001b[39m)\n\u001b[0;32m     43\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m load_scal(folder_names)\n\u001b[1;32m---> 45\u001b[0m train_loss, train_acc, validation_loss, validation_acc \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mTraining is done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Test Phase\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m, in \u001b[0;36mModelHandler.train\u001b[1;34m(self, train_loader, validation_loader)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m     23\u001b[0m                 correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#                   # getting the training set\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#                   data, target = data.to(device), target.to(device)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#                   # Get the model output (call the model with the data from this batch)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#                   output = self.model(data)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#                   # Zero the gradients out)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#                   self.optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#                   # Get the Loss\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#                   loss = self.criterion(output, target)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#                   # Calculate the gradients\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#                   loss.backward()\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#                   # Update the weights (using the training step of the optimizer)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#                   self.optimizer.step()\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#                   # Zero the gradients out\u001b[39;00m\n\u001b[0;32m     40\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m     42\u001b[0m                     \u001b[38;5;66;03m#getting the training set\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\PIL\\Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    865\u001b[0m ):\n\u001b[0;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\clone\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1550x550 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1550x550 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1550x550 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1550x550 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One modeling cycle\n",
    "\n",
    "for col, grp in grps.items():\n",
    "    \n",
    "    #Uncomment this to work on freq = 100 only for the saliency experiments\n",
    "#     freqs = [100]\n",
    "    for freq in tqdm(freqs):\n",
    "        \n",
    "#         df_done = downsample(freq)\n",
    "#         print(\"\\033[92mDownsampler is done!\")  \n",
    "#         CrtDir(grp = grp, col = col, exp = freq)\n",
    "#         print(\"\\033[92mCreating directories is done!\")\n",
    "#         slice_scal(freq = freq, df = df_done, w = 16.67, grp = grp, col = col, exp = freq)\n",
    "#         print(\"\\033[92mSlicing scalograms is done!\")\n",
    "#         crop_scal(grp = grp, exp = freq)\n",
    "#         print(\"\\033[92mCropping scalograms is done!\")\n",
    "    \n",
    "        for rn in randomlist:\n",
    "            print(freq)\n",
    "\n",
    "            random.seed(rn)\n",
    "            torch.manual_seed(rn)\n",
    "            torch.cuda.manual_seed_all(rn)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            np.random.seed(rn)\n",
    "            os.environ['PYTHONHASHSEED'] = str(rn)\n",
    "\n",
    "            folder_names = splt_scal(grp = grp, exp = freq)\n",
    "            print(\"\\033[92mTrain val test splitting is done!\")\n",
    "            \n",
    "            \n",
    "            train_loss, train_acc = [], []\n",
    "            validation_loss, validation_acc = [], []\n",
    "            \n",
    "            report_gpu() #Empties CUDA cache to deal with out of memory errors\n",
    "            \n",
    "            # Creating Model\n",
    "            net = b2b_net().to(device)\n",
    "            # net.load_state_dict(torch.load(f\"Models/{freq}_{col}_{rn}\"))\n",
    "            handler = ModelHandler(net, device, 30, 0.00005)\n",
    "            \n",
    "            train_loader, val_loader, test_loader = load_scal(folder_names)\n",
    "\n",
    "            train_loss, train_acc, validation_loss, validation_acc = handler.train(train_loader, val_loader)\n",
    "            print(\"\\033[92mTraining is done!\")\n",
    "\n",
    "            # Test Phase\n",
    "            test_acc = 0\n",
    "            output, target, test_acc = handler.test(test_loader)\n",
    "            print(\"\\033[92mTesting is done!\")\n",
    "\n",
    "            # # To CPU\n",
    "            # train_loss, train_acc = torch.tensor(train_loss).cpu(), torch.tensor(train_acc).cpu()\n",
    "            # validation_loss, validation_acc = torch.tensor(validation_loss).cpu(), torch.tensor(validation_acc).cpu()\n",
    "            \n",
    "            output, target = output.detach().clone().cpu(), target.detach().clone().cpu()\n",
    "            conf_mat = ConfusionMatrix(task=\"multiclass\", num_classes = len(grp))\n",
    "            conf_mat = conf_mat(output, target)\n",
    "            # print(conf_mat)\n",
    "            TP, FN, FP, TN = conf_mat[0][0], conf_mat[0][1], conf_mat[1][0], conf_mat[1][1]\n",
    "            \n",
    "            #Calculate some performance metrics\n",
    "            specificity = TN / (TN + FP)\n",
    "            sensitivity = TP / (TP + FN)\n",
    "            ppv         = TP / (TP + FP)\n",
    "            npv         = TN / (TN + FN)\n",
    "            \n",
    "            auroc = AUROC(task=\"multiclass\", num_classes = len(grp))\n",
    "            auprc = AveragePrecision(task=\"multiclass\", num_classes = len(grp))\n",
    "            multiclassresults[str(freq) + \"_\" + col + \"_\" + str(rn)] = [round(test_acc * 100, 2), \n",
    "                                                              auroc(output, target.type(torch.int64)),\n",
    "                                                              auprc(output, target.type(torch.int64)),\n",
    "                                                              specificity,\n",
    "                                                              sensitivity,\n",
    "                                                              ppv,\n",
    "                                                              npv                                                            \n",
    "            ]\n",
    "            \n",
    "            print(test_acc)\n",
    "\n",
    "            exp += 1\n",
    "            \n",
    "            \n",
    "            #Plot Epoch vs Accuracy & Epoch vs Loss\n",
    "            plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc, freq, col, rn)\n",
    "            \n",
    "            #Create the \"Models\" directory if it doesn't exist\n",
    "            if not os.path.exists(\"Models\"):\n",
    "                os.makedirs(\"Models\")\n",
    "                \n",
    "            #Save model weights and results    \n",
    "            torch.save(net.state_dict(), f\"Models/{freq}_{col}_{rn}\")\n",
    "            multiclassresults.to_csv(\"multiclassresults.csv\")\n",
    "            del net\n",
    "            print(f\"\\033[92mExp {exp - 1} is finally done!\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad970394",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_done\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_done' is not defined"
     ]
    }
   ],
   "source": [
    "df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f6bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df = pd.read_csv(\"multiclassresults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157ca49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>140_Age_200396</th>\n",
       "      <th>140_Age_544352</th>\n",
       "      <th>140_Age_524927</th>\n",
       "      <th>140_Age_635473</th>\n",
       "      <th>140_Age_609072</th>\n",
       "      <th>140_Age_680561</th>\n",
       "      <th>140_Age_227999</th>\n",
       "      <th>140_Age_156037</th>\n",
       "      <th>140_Age_535490</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>96.84</td>\n",
       "      <td>89.33</td>\n",
       "      <td>96.44</td>\n",
       "      <td>93.28</td>\n",
       "      <td>96.05</td>\n",
       "      <td>94.47</td>\n",
       "      <td>94.86</td>\n",
       "      <td>97.23</td>\n",
       "      <td>96.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9614)</td>\n",
       "      <td>tensor(0.8871)</td>\n",
       "      <td>tensor(0.9515)</td>\n",
       "      <td>tensor(0.9485)</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "      <td>tensor(0.9514)</td>\n",
       "      <td>tensor(0.9371)</td>\n",
       "      <td>tensor(0.9625)</td>\n",
       "      <td>tensor(0.9521)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tensor(0.9545)</td>\n",
       "      <td>tensor(0.8713)</td>\n",
       "      <td>tensor(0.9333)</td>\n",
       "      <td>tensor(0.9396)</td>\n",
       "      <td>tensor(0.9565)</td>\n",
       "      <td>tensor(0.9370)</td>\n",
       "      <td>tensor(0.9138)</td>\n",
       "      <td>tensor(0.9476)</td>\n",
       "      <td>tensor(0.9329)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tensor(0.9649)</td>\n",
       "      <td>tensor(0.8509)</td>\n",
       "      <td>tensor(0.9386)</td>\n",
       "      <td>tensor(0.8860)</td>\n",
       "      <td>tensor(0.9386)</td>\n",
       "      <td>tensor(0.9298)</td>\n",
       "      <td>tensor(0.9386)</td>\n",
       "      <td>tensor(0.9561)</td>\n",
       "      <td>tensor(0.9298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tensor(0.9712)</td>\n",
       "      <td>tensor(0.9281)</td>\n",
       "      <td>tensor(0.9856)</td>\n",
       "      <td>tensor(0.9712)</td>\n",
       "      <td>tensor(0.9784)</td>\n",
       "      <td>tensor(0.9568)</td>\n",
       "      <td>tensor(0.9568)</td>\n",
       "      <td>tensor(0.9856)</td>\n",
       "      <td>tensor(0.9856)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>tensor(0.9712)</td>\n",
       "      <td>tensor(0.8836)</td>\n",
       "      <td>tensor(0.9514)</td>\n",
       "      <td>tensor(0.9122)</td>\n",
       "      <td>tensor(0.9510)</td>\n",
       "      <td>tensor(0.9433)</td>\n",
       "      <td>tensor(0.9500)</td>\n",
       "      <td>tensor(0.9648)</td>\n",
       "      <td>tensor(0.9448)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>tensor(0.9649)</td>\n",
       "      <td>tensor(0.9065)</td>\n",
       "      <td>tensor(0.9817)</td>\n",
       "      <td>tensor(0.9619)</td>\n",
       "      <td>tensor(0.9727)</td>\n",
       "      <td>tensor(0.9464)</td>\n",
       "      <td>tensor(0.9469)</td>\n",
       "      <td>tensor(0.9820)</td>\n",
       "      <td>tensor(0.9815)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  140_Age_200396  140_Age_544352  140_Age_524927  140_Age_635473  \\\n",
       "0           0           96.84           89.33           96.44           93.28   \n",
       "1           1  tensor(0.9614)  tensor(0.8871)  tensor(0.9515)  tensor(0.9485)   \n",
       "2           2  tensor(0.9545)  tensor(0.8713)  tensor(0.9333)  tensor(0.9396)   \n",
       "3           3  tensor(0.9649)  tensor(0.8509)  tensor(0.9386)  tensor(0.8860)   \n",
       "4           4  tensor(0.9712)  tensor(0.9281)  tensor(0.9856)  tensor(0.9712)   \n",
       "5           5  tensor(0.9712)  tensor(0.8836)  tensor(0.9514)  tensor(0.9122)   \n",
       "6           6  tensor(0.9649)  tensor(0.9065)  tensor(0.9817)  tensor(0.9619)   \n",
       "\n",
       "   140_Age_609072  140_Age_680561  140_Age_227999  140_Age_156037  \\\n",
       "0           96.05           94.47           94.86           97.23   \n",
       "1  tensor(0.9667)  tensor(0.9514)  tensor(0.9371)  tensor(0.9625)   \n",
       "2  tensor(0.9565)  tensor(0.9370)  tensor(0.9138)  tensor(0.9476)   \n",
       "3  tensor(0.9386)  tensor(0.9298)  tensor(0.9386)  tensor(0.9561)   \n",
       "4  tensor(0.9784)  tensor(0.9568)  tensor(0.9568)  tensor(0.9856)   \n",
       "5  tensor(0.9510)  tensor(0.9433)  tensor(0.9500)  tensor(0.9648)   \n",
       "6  tensor(0.9727)  tensor(0.9464)  tensor(0.9469)  tensor(0.9820)   \n",
       "\n",
       "   140_Age_535490  \n",
       "0           96.05  \n",
       "1  tensor(0.9521)  \n",
       "2  tensor(0.9329)  \n",
       "3  tensor(0.9298)  \n",
       "4  tensor(0.9856)  \n",
       "5  tensor(0.9448)  \n",
       "6  tensor(0.9815)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079e33fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Classification</th>\n",
       "      <th>RN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>96.84</td>\n",
       "      <td>0.9614</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>Age</td>\n",
       "      <td>200396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>89.33</td>\n",
       "      <td>0.8871</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>Age</td>\n",
       "      <td>544352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140</td>\n",
       "      <td>96.44</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>Age</td>\n",
       "      <td>524927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>93.28</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>0.9396</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.9122</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>Age</td>\n",
       "      <td>635473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>96.05</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>Age</td>\n",
       "      <td>609072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Frequency Test_Accuracy   AUROC   AUPRC  Specificity  Sensitivity     PPV  \\\n",
       "0       140         96.84  0.9614  0.9545       0.9649       0.9712  0.9712   \n",
       "1       140         89.33  0.8871  0.8713       0.8509       0.9281  0.8836   \n",
       "2       140         96.44  0.9515  0.9333       0.9386       0.9856  0.9514   \n",
       "3       140         93.28  0.9485  0.9396       0.8860       0.9712  0.9122   \n",
       "4       140         96.05  0.9667  0.9565       0.9386       0.9784  0.9510   \n",
       "\n",
       "      NPV Classification      RN  \n",
       "0  0.9649            Age  200396  \n",
       "1  0.9065            Age  544352  \n",
       "2  0.9817            Age  524927  \n",
       "3  0.9619            Age  635473  \n",
       "4  0.9727            Age  609072  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame(result).T.reset_index()\n",
    "res.columns = ['Frequency', 'Test_Accuracy', 'AUROC', 'AUPRC','Specificity', \"Sensitivity\", \"PPV\", \"NPV\"]\n",
    "res = res.drop(0)\n",
    "res = res.iloc[0:].reset_index(drop=True)\n",
    "res[\"Frequency\"], res[\"Classification\"], res['RN'] = ([c.split(\"_\")[0] for c in res.Frequency],\n",
    "                                                      [d.split(\"_\")[1] for d in  res.Frequency], \n",
    "                                                      [e.split(\"_\")[2] for e in res.Frequency])\n",
    "    \n",
    "\n",
    "res['AUROC'], res['AUPRC'], res['Specificity'], res[\"Sensitivity\"], res[\"PPV\"], res[\"NPV\"] = ([float(str(c).split(\"(\")[1][:-1]) for c in res['AUROC']],\n",
    "                                                                                [float(str(c).split(\"(\")[1][:-1]) for c in res['AUPRC']],\n",
    "                                                                                [float(str(c).split(\"(\")[1][:-1]) for c in res['Specificity']],\n",
    "                                                                                [float(str(c).split(\"(\")[1][:-1]) for c in res['Sensitivity']],\n",
    "                                                                                [float(str(c).split(\"(\")[1][:-1]) for c in res['PPV']], \n",
    "                                                                                [float(str(c).split(\"(\")[1][:-1]) for c in res['NPV']])\n",
    "\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfda09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Classification</th>\n",
       "      <th>RN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>96.84</td>\n",
       "      <td>0.9614</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>Age</td>\n",
       "      <td>200396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>89.33</td>\n",
       "      <td>0.8871</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>Age</td>\n",
       "      <td>544352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140</td>\n",
       "      <td>96.44</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>Age</td>\n",
       "      <td>524927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>93.28</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>0.9396</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.9122</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>Age</td>\n",
       "      <td>635473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>96.05</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>Age</td>\n",
       "      <td>609072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>140</td>\n",
       "      <td>94.47</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>0.9370</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.9433</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>Age</td>\n",
       "      <td>680561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>140</td>\n",
       "      <td>94.86</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>0.9138</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>0.9469</td>\n",
       "      <td>Age</td>\n",
       "      <td>227999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>140</td>\n",
       "      <td>97.23</td>\n",
       "      <td>0.9625</td>\n",
       "      <td>0.9476</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>Age</td>\n",
       "      <td>156037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>140</td>\n",
       "      <td>96.05</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.9329</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>Age</td>\n",
       "      <td>535490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Frequency Test_Accuracy   AUROC   AUPRC  Specificity  Sensitivity     PPV  \\\n",
       "0       140         96.84  0.9614  0.9545       0.9649       0.9712  0.9712   \n",
       "1       140         89.33  0.8871  0.8713       0.8509       0.9281  0.8836   \n",
       "2       140         96.44  0.9515  0.9333       0.9386       0.9856  0.9514   \n",
       "3       140         93.28  0.9485  0.9396       0.8860       0.9712  0.9122   \n",
       "4       140         96.05  0.9667  0.9565       0.9386       0.9784  0.9510   \n",
       "5       140         94.47  0.9514  0.9370       0.9298       0.9568  0.9433   \n",
       "6       140         94.86  0.9371  0.9138       0.9386       0.9568  0.9500   \n",
       "7       140         97.23  0.9625  0.9476       0.9561       0.9856  0.9648   \n",
       "8       140         96.05  0.9521  0.9329       0.9298       0.9856  0.9448   \n",
       "\n",
       "      NPV Classification      RN  \n",
       "0  0.9649            Age  200396  \n",
       "1  0.9065            Age  544352  \n",
       "2  0.9817            Age  524927  \n",
       "3  0.9619            Age  635473  \n",
       "4  0.9727            Age  609072  \n",
       "5  0.9464            Age  680561  \n",
       "6  0.9469            Age  227999  \n",
       "7  0.9820            Age  156037  \n",
       "8  0.9815            Age  535490  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296ab5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#            'AUROC mean', 'AUROC CI-upper', 'AUROC CI lower',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPV mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPV CI-upper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPV CI-lower\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNPV mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNPV CI-upper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNPV CI-lower\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m resavg \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m columns)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m freq \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfreqs\u001b[49m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col, grp \u001b[38;5;129;01min\u001b[39;00m grps\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     14\u001b[0m         resavg\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m freq\n",
      "\u001b[1;31mNameError\u001b[0m: name 'freqs' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "columns = [\n",
    "           'Frequency', 'Classification',\n",
    "#            'AUROC mean', 'AUROC CI-upper', 'AUROC CI lower',\n",
    "#            'AUPRC mean', 'AUPRC CI-upper', 'AUPRC CI lower',\n",
    "           'Specificity mean', 'Specificity CI-upper', 'Specificity CI-lower',\n",
    "           'Sensitivity mean', 'Sensitivity CI-upper', 'Sensitivity CI-lower',\n",
    "           'PPV mean', 'PPV CI-upper', 'PPV CI-lower',\n",
    "           'NPV mean', 'NPV CI-upper', 'NPV CI-lower']\n",
    "\n",
    "resavg = pd.DataFrame(columns = columns)\n",
    "for freq in freqs:\n",
    "    for col, grp in grps.items():\n",
    "        resavg.loc[i, 'Frequency'] = freq\n",
    "        resavg.loc[i, 'Classification'] = col\n",
    "        \n",
    "#         x = res[(res['Frequency'] == str(freq)) & (res['Classification'] == col)].loc[:, 'AUROC']\n",
    "#         resavg.loc[i, 'AUROC mean'] = x.mean() * 100\n",
    "#         resavg.loc[i, 'AUROC CI-upper'], resavg.loc[i, 'AUROC CI-lower'] = sp.stats.t.interval(confidence=0.95,\n",
    "#                                                                                                 df=len(x)-1, loc=np.mean(x), scale=sp.stats.sem(x))\n",
    "#         resavg.loc[i, 'AUROC CI-upper'], resavg.loc[i, 'AUROC CI-lower'] = (resavg.loc[i, 'AUROC CI-upper'] * 100,\n",
    "#                                                                                         resavg.loc[i, 'AUROC CI-lower'] * 100)\n",
    "        \n",
    "#         x = res[(res['Frequency'] == str(freq)) & (res['Classification'] == col)].loc[:, 'AUPRC']\n",
    "#         resavg.loc[i, 'AUPRC mean'] = x.mean() * 100\n",
    "#         resavg.loc[i, 'AUPRC CI-upper'], resavg.loc[i, 'AUPRC CI-lower'] = sp.stats.t.interval(confidence=0.95,\n",
    "#                                                                                                df=len(x)-1, loc=np.mean(x), scale=sp.stats.sem(x))\n",
    "#         resavg.loc[i, 'AUPRC CI-upper'], resavg.loc[i, 'AUPRC CI-lower'] = (resavg.loc[i, 'AUPRC CI-upper'] * 100,\n",
    "#                                                                                         resavg.loc[i, 'AUPRC CI-lower'] * 100)\n",
    "\n",
    "        \n",
    "        x = res[(res['Frequency'] == str(freq)) & (res['Classification'] == col)].loc[:, 'Specificity']\n",
    "        resavg.loc[i, 'Specificity mean'] = x.mean() * 100\n",
    "        resavg.loc[i, 'Specificity CI-upper'], resavg.loc[i, 'Specificity CI-lower'] = sp.stats.t.interval(confidence=0.95,\n",
    "                                                                                                df=len(x)-1, loc=np.mean(x), scale=sp.stats.sem(x))\n",
    "        resavg.loc[i, 'Specificity CI-upper'], resavg.loc[i, 'Specificity CI-lower'] = (resavg.loc[i, 'Specificity CI-upper'] * 100,\n",
    "                                                                                        resavg.loc[i, 'Specificity CI-lower'] * 100)\n",
    "        \n",
    "        \n",
    "        x = res[(res['Frequency'] == str(freq)) & (res['Classification'] == col)].loc[:, 'Sensitivity']\n",
    "        resavg.loc[i, 'Sensitivity mean'] = x.mean() * 100\n",
    "        resavg.loc[i, 'Sensitivity CI-upper'], resavg.loc[i, 'Sensitivity CI-lower'] = sp.stats.t.interval(confidence=0.95,\n",
    "                                                                                                df=len(x)-1, loc=np.mean(x), scale=sp.stats.sem(x))\n",
    "        resavg.loc[i, 'Sensitivity CI-upper'], resavg.loc[i, 'Sensitivity CI-lower'] = (resavg.loc[i, 'Sensitivity CI-upper'] * 100,\n",
    "                                                                                        resavg.loc[i, 'Sensitivity CI-lower'] * 100)\n",
    "        \n",
    "        x = res[(res['Frequency'] == str(freq)) & (res['Classification'] == col)].loc[:, 'PPV']\n",
    "        resavg.loc[i, 'PPV mean'] = x.mean() * 100\n",
    "        resavg.loc[i, 'PPV CI-upper'], resavg.loc[i, 'PPV CI-lower'] = sp.stats.t.interval(confidence=0.95,\n",
    "                                                                                                df=len(x)-1, loc=np.mean(x), scale=sp.stats.sem(x))\n",
    "        resavg.loc[i, 'PPV CI-upper'], resavg.loc[i, 'PPV CI-lower'] = (resavg.loc[i, 'PPV CI-upper'] * 100,\n",
    "                                                                        resavg.loc[i, 'PPV CI-lower'] * 100)\n",
    "        \n",
    "        \n",
    "        x = res[(res['Frequency'] == str(freq)) & (res['Classification'] == col)].loc[:, 'NPV']\n",
    "        resavg.loc[i, 'NPV mean'] = x.mean() * 100\n",
    "        resavg.loc[i, 'NPV CI-upper'], resavg.loc[i, 'NPV CI-lower'] = sp.stats.t.interval(confidence=0.95,\n",
    "                                                                                                df=len(x)-1, loc=np.mean(x), scale=sp.stats.sem(x))\n",
    "        resavg.loc[i, 'NPV CI-upper'], resavg.loc[i, 'NPV CI-lower'] = (resavg.loc[i, 'NPV CI-upper'] * 100,\n",
    "                                                                        resavg.loc[i, 'NPV CI-lower'] * 100)\n",
    "        \n",
    "        i += 1\n",
    "resavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bae9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Figures directory if it does not exist\n",
    "if not os.path.exists(\"Figures\"):\n",
    "    os.mkdir(\"Figures\")\n",
    "\n",
    "\n",
    "def plot_line(metric):\n",
    "    plt.plot( x['Frequency'].astype('float'), x[f'{metric} mean'], label = f'{metric}')\n",
    "    plt.fill_between(x['Frequency'].astype('float'), \n",
    "                     x[f'{metric} CI-lower'].astype('float'), x[f'{metric} CI-upper'].astype('float'), alpha=.1)\n",
    "\n",
    "for col in ['Age', 'Sex', 'Diet', 'SexDiet', 'AgeDiet', 'SexAge', 'SexAgeDiet']:\n",
    "    \n",
    "    x = resavg[resavg.Classification == col]\n",
    "    plot_line(\"Sensitivity\")\n",
    "    plot_line(\"Specificity\")\n",
    "    plot_line(\"PPV\")\n",
    "    plot_line(\"NPV\")\n",
    "#     plt.plot( x['Frequency'].astype('float'), x['AUROC mean'], label = 'AUROC')\n",
    "#     plt.fill_between(x['Frequency'].astype('float'), \n",
    "#                      x['AUROC CI-lower'].astype('float'), x['AUROC CI-upper'].astype('float'), alpha=.1)\n",
    "    \n",
    "#     plt.plot( x['Frequency'].astype('float'), x['AUPRC mean'], label = 'AUPRC')\n",
    "#     plt.fill_between(x['Frequency'].astype('float'), \n",
    "#                      x['AUPRC CI-lower'].astype('float'), x['AUPRC CI-upper'].astype('float'), alpha=.1)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.title(f\"{col} classification by increasing sample rate\")\n",
    "    plt.savefig(f\"Figures\\class_{col}.png\")\n",
    "    plt.clf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93da844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Generating saliency maps for each classification at each freq\n",
    "\n",
    "freq_list = [140, 120, 100, 80, 70, 60, 50, 40]\n",
    "for freq in freq_list:\n",
    "    for col, grp in grps.items():\n",
    "        for rn in randomlist:\n",
    "            folder_names = splt_scal(grp = grp, exp = freq)\n",
    "            model = b2b_net().to(device)\n",
    "            model.load_state_dict(torch.load(f\"Models/{freq}_{col}_{rn}\"))\n",
    "            handler = ModelHandler(model, device, 30, 0.00005)\n",
    "\n",
    "            _, _, test_loader = load_scal(folder_names, batch_size=1, shuffle=False)\n",
    "            name_dic = create_name_dic(freq=freq, col=col)\n",
    "            handler.visualize(test_loader, name_dic, freq, col, rn)\n",
    "            print(f\"\\033[92m{col} visualization is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop the produced saliency maps to remove all white spaces...\n",
    "# Example: [left, upper, right, lower] for a 6.4 x 4.8 inch image (aspect ratio 4:3)\n",
    "\n",
    "box = [129, 58, 527, 427]\n",
    "\n",
    "sal = \"Saliency\"\n",
    "\n",
    "freq_list = [140, 120, 100, 80, 70, 60, 50, 40]\n",
    "for freq in freq_list:\n",
    "    for col, _ in tqdm(grps.items()):\n",
    "        for rn in randomlist:\n",
    "            folder = os.path.join(sal, f\"{freq}_{col}_{rn}\")\n",
    "            for file in os.listdir(folder):\n",
    "                img1 = Image.open(os.path.join(folder, file))\n",
    "                img1 = img1.crop(box)\n",
    "                img1.save(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121affff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fn that concatenates two images together\n",
    "def get_concat_h(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst\n",
    "\n",
    "\n",
    "#Create new folders that contain saliency-scalogram pairs\n",
    "##UPDATE THIS CODE TO REFLECT THE PRESENCE OF 10 FOLDERS PER CLASSIFICATION\n",
    "##ALSO REMEMBER TO ADD THE ALL_SCAL FOLDER THAN CONTAINS ALL SCALOGRAMS IN A GIVEN CLASSIFICATION\n",
    "\n",
    "# for i in range(len(os.listdir(\"Saliency\")) - 1):\n",
    "#     main = \"Saliency-scalograms\"\n",
    "#     path = os.path.join(main, os.listdir(\"Saliency\")[i])\n",
    "#     if not os.path.exists(main):\n",
    "#         os.mkdir(main)\n",
    "#     if not os.path.exists(path):\n",
    "#         os.mkdir(path)\n",
    "\n",
    "##############\n",
    "'''\n",
    "REMEMBER TO COPY ALL SCALOGRAMS PER CLASSIFICATION INTO THE FOLDER THAT WILL CONTAINS ALL SCALOGRAMS\n",
    "THOSE WILL BE USED FOR SALIENCY-SCALOGRAM CONCATENATION\n",
    "'''\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94229982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCATENATE SCALOGRAMS WITH THEIR RESPECTIVE SALIENCY MAPS\n",
    "\n",
    "freq_list = [140, 120, 100, 80, 70, 60, 50, 40]\n",
    "base_path = \"D:/Saliency-scalograms\"  # Change the base path to the new directory\n",
    "\n",
    "for freq in freq_list:\n",
    "    for col, _ in grps.items():\n",
    "        scal = os.path.join(\"Scalograms2\", f\"freq_{freq}_({col})\", \"scal-cropped\", \"all_scal\")\n",
    "        for rn in randomlist:\n",
    "            sal = f\"Saliency/{freq}_{col}_{rn}\"\n",
    "            salscal = os.path.join(base_path, f\"{freq}_{col}_{rn}\")  # Updated salscal path\n",
    "\n",
    "            # Create salscal if it is not available\n",
    "            if not os.path.exists(salscal):\n",
    "                os.makedirs(salscal)\n",
    "            for j in range(len(os.listdir(sal))):\n",
    "                img1 = Image.open(os.path.join(sal, os.listdir(sal)[j]))\n",
    "                img2 = Image.open(os.path.join(scal, os.listdir(sal)[j]))\n",
    "                get_concat_h(img1, img2).save(os.path.join(salscal, os.listdir(sal)[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_salscal = \"Figures/avg_salscal\"\n",
    "\n",
    "#Create a new folder to save average saliency-scalogram pairs for each model\n",
    "if not os.path.exists(avg_salscal):\n",
    "            os.mkdir(avg_salscal)\n",
    "\n",
    "base_path = \"D:/Saliency-scalograms\" \n",
    "\n",
    "freq_list = [140, 120, 100, 80, 70, 60, 50, 40]\n",
    "for freq in freq_list:\n",
    "    for col, _ in grps.items():\n",
    "        for rn in randomlist:\n",
    "            salscal = os.path.join(base_path, f\"{freq}_{col}_{rn}\")\n",
    "            avg_array = np.zeros((369, 794, 3))\n",
    "            for file in os.listdir(salscal):\n",
    "                img_path = os.path.join(salscal, file)\n",
    "                img = Image.open(img_path)\n",
    "                array = np.array(img)\n",
    "                avg_array += array\n",
    "            avg_array = (avg_array/len(os.listdir(salscal))).astype(\"uint8\")\n",
    "            avg_image = Image.fromarray(avg_array)\n",
    "            avg_image.save(os.path.join(avg_salscal, f\"{freq}_{col}_{rn}_avg_sal.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Classification', 'Rn'] + list(range(369))\n",
    "# index = list(np.arange)\n",
    "results_sal = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662b4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "freq_list = [140, 120, 100, 80, 70, 60, 50, 40]\n",
    "\n",
    "for freq in freq_list:\n",
    "    for col, _ in grps.items():\n",
    "        for rn in randomlist:    \n",
    "            sal = f\"Saliency/{freq}_{col}_{rn}\"    \n",
    "            avg_freq = np.zeros((369, ))\n",
    "            for file in os.listdir(sal):\n",
    "                img_path = os.path.join(sal, file)\n",
    "                img = Image.open(img_path)\n",
    "                array = np.array(img).reshape(369, 1592)\n",
    "                for i in range(1592):    \n",
    "                    avg_freq += array[:, i]\n",
    "            denominator = len(os.listdir(sal)) * 1592\n",
    "            avg_freq = (avg_freq / denominator)\n",
    "\n",
    "            #Store the average saliency results for each model\n",
    "            results_sal.loc[j, 'Classification'] = col\n",
    "            results_sal.loc[j, 'Rn'] = rn\n",
    "            results_sal.loc[j, list(range(369))] = avg_freq[::-1]\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d48176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average saliency of models per classification\n",
    "\n",
    "j = 30\n",
    "for col, _                        in grps.items():\n",
    "        x = results_sal[(results_sal['Classification'] == col) &\n",
    "                        (results_sal['Rn'].isin(randomlist))].iloc[:, 2:]\n",
    "        #Create a row to store the mean saliency of classification, and calculate it\n",
    "        results_sal.loc[j, \"Classification\"] = col\n",
    "        results_sal.loc[j, \"Rn\"] = \"mean\"\n",
    "        results_sal.loc[j, np.arange(369)] = x.mean(axis=0)\n",
    "        j += 1\n",
    "        #Create two rows to store CI-upper and lower\n",
    "        results_sal.loc[j, \"Classification\"] = col\n",
    "        results_sal.loc[j, \"Rn\"] = \"CI-upper\"\n",
    "        j += 1\n",
    "        results_sal.loc[j, \"Classification\"] = col\n",
    "        results_sal.loc[j, \"Rn\"] = \"CI-lower\"\n",
    "        #Calculate the confidence intervals for each classification\n",
    "        for i in range(369):\n",
    "            x_column = x.iloc[:30, i]\n",
    "            # print(x_column)\n",
    "            results_sal.iloc[j - 1, 2 + i], results_sal.iloc[j, 2 + i] = sp.stats.t.interval(confidence=0.95,\n",
    "                                                                                 df=len(x_column)-1, loc=np.mean(x_column), scale=sp.stats.sem(x_column))\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Age', 'Sex', 'Diet', 'SexDiet', 'AgeDiet', 'SexAge', 'SexAgeDiet']:\n",
    "    \n",
    "    x = results_sal[results_sal.Classification == col]\n",
    "    plt.plot(np.linspace(0, 50, 369), x[x.Rn == 'mean'].iloc[:, 2:].to_numpy(float).flatten(), label = col)\n",
    "    plt.fill_between(np.linspace(0, 50, 369),\n",
    "                     x[x.Rn == 'CI-lower'].iloc[:, 2:].to_numpy(float).flatten(),\n",
    "                     x[x.Rn == 'CI-upper'].iloc[:, 2:].to_numpy(float).flatten(), alpha=.1)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Time averaged Saliency (a.u.)\")\n",
    "    plt.legend(title=\"Classification\")\n",
    "    plt.savefig(f\"Figures/Time-averaged {col} Saliency.png\")\n",
    "    plt.clf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff160f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Age', 'Sex', 'Diet', 'SexDiet', 'AgeDiet', 'SexAge', 'SexAgeDiet']:\n",
    "    \n",
    "    x = results_sal[results_sal.Classification == col]\n",
    "    mean = sp.signal.resample(x[x.Rn == 'mean'].iloc[:, 4:].to_numpy(float).flatten(), 50)\n",
    "    up_ci = sp.signal.resample(x[x.Rn == 'CI-upper'].iloc[:, 4:].to_numpy(float).flatten(), 50)\n",
    "    low_ci = sp.signal.resample(x[x.Rn == 'CI-lower'].iloc[:, 4:].to_numpy(float).flatten(), 50)\n",
    "    \n",
    "    plt.plot(np.arange(1, 51), mean)\n",
    "    mean_max, _ = sp.signal.find_peaks(mean, distance=4, prominence = .35)\n",
    "    mean_max = mean_max[:5]\n",
    "    plt.scatter(mean_max + 1, mean[mean_max], s=50, marker='o', facecolor='none', edgecolor='red', label = mean_max )\n",
    "    \n",
    "    plt.fill_between(np.arange(1, 51), low_ci, up_ci, alpha=.1)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Time averaged Saliency (a.u.)\")\n",
    "    plt.legend(title=\"Frequency peaks\")\n",
    "    plt.title(f\"{col} saliency\")\n",
    "#     plt.savefig(f\"Figures/Time-averaged {col} Saliency resampled.png\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print top 5 mean frequencies whose saliency was highest for each classification\n",
    "for col in ['Age', 'Sex', 'Diet', 'SexDiet', 'AgeDiet', 'SexAge', 'SexAgeDiet']:\n",
    "    x = results_sal[results_sal.Classification == col]\n",
    "    x = sp.signal.resample(x[x.Rn == 'mean'].iloc[:, 2:].to_numpy(float).flatten(), 50)\n",
    "    #Print indices of top 5 saliencies, which correspond to frequencies when divided by 50\n",
    "    print(f\"Top 5 {col} frequencies with highest mean time-averaged saliencies\", np.argsort(-x)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ff2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty figure for editing purposes\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 5)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yticks(np.arange(0, 51, 10))\n",
    "plt.show\n",
    "plt.savefig(\"blank.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15863a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Single image cropping trial\n",
    "\n",
    "img_path = 'Scalograms2/freq_60_(Age)/scal/12 wks/12 wks0000.png'\n",
    "image = Image.open(img_path)\n",
    "image\n",
    "\n",
    "box = [81, 59, 477, 427] #previously, 3rd number was 390\n",
    "region = image.crop(box)\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dummy spectrogram with a colorbar for editing purposes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.get_cmap('viridis')\n",
    "# if x.iloc[i * w:(i + 1)* w].isna().values.any() == False:\n",
    "pxx,  freq, t, cax = plt.specgram(x[:1000], Fs=50)\n",
    "fig.colorbar(cax)\n",
    "plt.savefig('samplespec.png', dpi = 300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.arange(1, 100), np.arange(1, 100)\n",
    "\n",
    "plt.plot(a * b[::-1], a / b[::-1]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
